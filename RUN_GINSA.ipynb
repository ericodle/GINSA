{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5ph71fSlH/JN3YD0N2bSn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericodle/GINSA/blob/main/RUN_GINSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Required Packages"
      ],
      "metadata": {
        "id": "EJgaQVkksa5x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnxiphLXmtxY",
        "outputId": "01bb93c0-a9f1-4d66-aa60-e97d881f0131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=d24dffbc7b14265ee19eee810b3da3521efe2e02204e639a758d41bf013b2ae6\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.23.5)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.81\n"
          ]
        }
      ],
      "source": [
        "!pip install requests\n",
        "!pip install wget\n",
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dependencies"
      ],
      "metadata": {
        "id": "hhFmjDUFshQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import json\n",
        "import urllib.parse\n",
        "import os\n",
        "import csv\n",
        "import requests\n",
        "import pandas as pd\n",
        "import wget\n",
        "import gzip\n",
        "import shutil\n",
        "from Bio import SeqIO\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Wtoaxix4m5Uo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Core Functions"
      ],
      "metadata": {
        "id": "jfhEQvnxslj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################################################################\n",
        "\n",
        "def search_species_occurrences(species_name, limit=1000):\n",
        "    base_url = \"https://api.gbif.org/v1/occurrence/search\"\n",
        "    params = {\n",
        "        \"scientificName\": species_name,\n",
        "        \"limit\": limit,\n",
        "    }\n",
        "\n",
        "    occurrence_ids = []\n",
        "\n",
        "    offset = 0\n",
        "    while True:\n",
        "        params[\"offset\"] = offset\n",
        "        encoded_params = urllib.parse.urlencode(params)\n",
        "        full_url = f\"{base_url}?{encoded_params}\"\n",
        "\n",
        "        with urllib.request.urlopen(full_url) as response:\n",
        "            data = json.loads(response.read())\n",
        "\n",
        "            if \"results\" in data:\n",
        "                occurrence_ids.extend([occurrence[\"key\"] for occurrence in data[\"results\"]])\n",
        "                if len(data[\"results\"]) < limit:\n",
        "                    break  # Reached the end of results\n",
        "                else:\n",
        "                    offset += limit\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    return occurrence_ids\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def create_folders(occurrence_ids, proj_dir):\n",
        "    if not os.path.exists(proj_dir):\n",
        "        os.makedirs(proj_dir)\n",
        "\n",
        "    for occurrence_id in occurrence_ids:\n",
        "        folder_path = os.path.join(proj_dir, str(occurrence_id))\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def generate_csv(occurrence_ids, proj_dir):\n",
        "    with open(proj_dir + '/occurrences.csv', mode='w', newline='', encoding='utf-8') as csv_file:\n",
        "        fieldnames = ['occurrence_id', 'country', 'latitude', 'longitude', 'prefix_text']\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "\n",
        "        for occurrence_id in occurrence_ids:\n",
        "            url = f\"https://api.gbif.org/v1/occurrence/{occurrence_id}\"\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                try:\n",
        "                    data = response.json()\n",
        "                    long_id = data.get(\"occurrenceID\")\n",
        "                    if long_id:\n",
        "                        country = data.get(\"countryCode\")\n",
        "                        latitude = data.get(\"decimalLatitude\")\n",
        "                        longitude = data.get(\"decimalLongitude\")\n",
        "                        underscore_index = long_id.find(\"_\")\n",
        "                        if underscore_index != -1:\n",
        "                            prefix_text = long_id[:underscore_index]\n",
        "                        else:\n",
        "                            prefix_text = \"N/A\"\n",
        "                        writer.writerow({\n",
        "                            'occurrence_id': occurrence_id,\n",
        "                            'country': country,\n",
        "                            'latitude': latitude,\n",
        "                            'longitude': longitude,\n",
        "                            'prefix_text': prefix_text\n",
        "                        })\n",
        "                    else:\n",
        "                        print(f\"occurrenceID not found for occurrence ID {occurrence_id}. Skipping.\")\n",
        "\n",
        "                except ValueError:\n",
        "                    print(f\"Failed to parse JSON for occurrence {occurrence_id}.\")\n",
        "            else:\n",
        "                print(f\"Failed to fetch content for occurrence {occurrence_id}. Status code: {response.status_code}\")\n",
        "\n",
        "    print(\"CSV file created successfully.\")\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def process_directory(proj_dir):\n",
        "    for root, dirs, files in os.walk(proj_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".gz\"):\n",
        "                gz_file_path = os.path.join(root, file)\n",
        "                extract_path = os.path.splitext(gz_file_path)[0]\n",
        "\n",
        "                with gzip.open(gz_file_path, 'rb') as f_in, open(extract_path, 'wb') as f_out:\n",
        "                    shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "                os.remove(gz_file_path)\n",
        "                print(f\"Extracted and deleted: {gz_file_path}\")\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def ssu_fasta_grab(csv_file, proj_dir):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Replace with your actual MGnify API endpoint\n",
        "    api_base_url = \"https://www.ebi.ac.uk/metagenomics/api/v1\"\n",
        "\n",
        "    # Iterate through the DataFrame and process each occurrence\n",
        "    for prefix_text, occurrence_id in zip(df['prefix_text'], df['occurrence_id']):\n",
        "        if pd.isna(prefix_text):  # Check if prefix_text is NaN\n",
        "            print(f\"MGnify link not found in occurrence metadata for ID {occurrence_id}.\")\n",
        "            continue  # Continue to the next iteration\n",
        "\n",
        "        if \"MGY\" not in prefix_text.upper():  # Confirm prefix text points to EMBI MGnify\n",
        "            print(f\"MGnify link not found in occurrence metadata for ID {occurrence_id}.\")\n",
        "            continue  # Continue to the next iteration\n",
        "\n",
        "        url = f\"{api_base_url}/analyses/{prefix_text}/downloads\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "\n",
        "        print(f\"Processing {prefix_text}...\")\n",
        "\n",
        "        # Create a directory with the occurrence_id in the project directory\n",
        "        subdir_path = os.path.join(proj_dir, str(occurrence_id))\n",
        "        if not os.path.exists(subdir_path):\n",
        "            os.makedirs(subdir_path)\n",
        "\n",
        "        fasta_links = []\n",
        "\n",
        "        for entry in data['data']:\n",
        "            link_entry = entry['links']['self']\n",
        "            if link_entry.endswith(\"SSU.fasta.gz\"):\n",
        "                fasta_links.append(link_entry)\n",
        "\n",
        "        if fasta_links:\n",
        "            print(f\"Downloading {len(fasta_links)} file(s) for {prefix_text}...\")\n",
        "            for fasta_link in fasta_links:\n",
        "                file_name = os.path.basename(fasta_link)\n",
        "                file_path = os.path.join(subdir_path, file_name)\n",
        "                wget.download(fasta_link, file_path)\n",
        "            print(\" Download complete.\")\n",
        "        else:\n",
        "            print(f\"No fasta files found for {prefix_text}. Saving 'no_fasta.txt'...\")\n",
        "            no_fasta_file = os.path.join(subdir_path, \"no_fasta.txt\")\n",
        "            with open(no_fasta_file, \"w\") as f:\n",
        "                f.write(\"No fasta files found.\")\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def mapseq_grab(csv_file, proj_dir):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Replace with your actual MGnify API endpoint\n",
        "    api_base_url = \"https://www.ebi.ac.uk/metagenomics/api/v1\"\n",
        "\n",
        "    # Iterate through the DataFrame and process each occurrence\n",
        "    for prefix_text, occurrence_id in zip(df['prefix_text'], df['occurrence_id']):\n",
        "        if pd.isna(prefix_text):  # Check if prefix_text is NaN\n",
        "            print(f\"MGnify link not found in occurrence metadata for ID {occurrence_id}.\")\n",
        "            continue  # Continue to the next iteration\n",
        "\n",
        "        if \"MGY\" not in prefix_text.upper():  # Confirm prefix text points to EMBI MGnify\n",
        "            print(f\"MGnify link not found in occurrence metadata for ID {occurrence_id}.\")\n",
        "            continue  # Continue to the next iteration\n",
        "\n",
        "        url = f\"{api_base_url}/analyses/{prefix_text}/downloads\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "\n",
        "        print(f\"Processing {prefix_text}...\")\n",
        "\n",
        "        # Create a directory with the occurrence_id in the project directory\n",
        "        subdir_path = os.path.join(proj_dir, str(occurrence_id))\n",
        "        if not os.path.exists(subdir_path):\n",
        "            os.makedirs(subdir_path)\n",
        "\n",
        "        mapseq_links = []\n",
        "\n",
        "\n",
        "        # Check if 'data' key is present in the response dictionary\n",
        "        if 'data' not in data:\n",
        "            print(\"Error: 'data' key not present in the API response.\")\n",
        "            return\n",
        "\n",
        "        # Loop through the data to find links ending with \"MERGED_FASTQ_SSU_MAPSeq.mseq.gz\"\n",
        "        for entry in data['data']:\n",
        "            link_entry = entry['links']['self']\n",
        "            if link_entry.endswith(\"SSU_MAPSeq.mseq.gz\"):\n",
        "                mapseq_links.append(link_entry)\n",
        "\n",
        "\n",
        "        if mapseq_links:\n",
        "            print(f\"Downloading {len(mapseq_links)} file(s) for {occurrence_id}...\")\n",
        "            for mapseq_link in mapseq_links:\n",
        "                file_name = os.path.basename(mapseq_link)\n",
        "                file_path = os.path.join(subdir_path, file_name)\n",
        "                wget.download(mapseq_link, file_path)\n",
        "            print(\"Download complete.\")\n",
        "        else:\n",
        "            print(f\"No MAPSeq files found for {occurrence_id}. Saving 'no_mapseq.txt'...\")\n",
        "            no_mapseq_file = os.path.join(subdir_path, \"no_mapseq.txt\")\n",
        "            with open(no_mapseq_file, \"w\") as f:\n",
        "                f.write(\"No MAPSeq files found.\")\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def find_target_in_mapseq(subdirectory, file_path, genus, species):\n",
        "\n",
        "    truncated_strings = []  # List to store truncated strings\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            content = file.readlines()\n",
        "\n",
        "            for line in content:\n",
        "                line = line.strip()\n",
        "                if genus in line and species in line:\n",
        "                    # Find the index of the search word in the line\n",
        "                    word_index = line.index(species)\n",
        "\n",
        "                    # Extract the preceding string\n",
        "                    preceding_string = line[:word_index].strip()\n",
        "                    # Truncate to only the text before the first space\n",
        "                    truncated_string = preceding_string.split()[0]\n",
        "                    truncated_strings.append(truncated_string)\n",
        "\n",
        "        with open(subdirectory+\"/truncated_strings.txt\", 'w') as output_file:\n",
        "            for item in truncated_strings:\n",
        "                output_file.write(\"%s\\n\" % item)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"MAPSeq Search Error: The file does not exist or the path is incorrect.\")\n",
        "\n",
        "    return truncated_strings\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def sift_fasta(fasta_file_path, truncated_strings_path):\n",
        "    sifted_data = {}  # Dictionary to store truncated_strings and their corresponding DNA sequences\n",
        "\n",
        "    if truncated_strings_path:\n",
        "        with open(truncated_strings_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "        lines = [line.strip() for line in lines]\n",
        "\n",
        "# Parse the FASTA file and extract sequences\n",
        "    with open(fasta_file_path, \"rt\") as fasta_file:\n",
        "        records = SeqIO.parse(fasta_file, \"fasta\")\n",
        "        for record in records:\n",
        "            sequence_name = record.id\n",
        "            sequence = str(record.seq)\n",
        "\n",
        "# Search for sequence names in the truncated_strings list\n",
        "            for truncated_string in lines:\n",
        "                if truncated_string == sequence_name:\n",
        "                    sifted_data.setdefault(truncated_string, []).append(sequence)\n",
        "                    break\n",
        "\n",
        "    return sifted_data\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def save_sifted_data_to_csv(sifted_data, output_file):\n",
        "    with open(output_file, \"w\", newline=\"\") as csvfile:\n",
        "        csv_writer = csv.writer(csvfile)\n",
        "        csv_writer.writerow([\"Truncated String\", \"Sequences\"])\n",
        "\n",
        "        for truncated_string, sequences in sifted_data.items():\n",
        "            csv_writer.writerow([truncated_string, \"\\n\".join(sequences)])\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def analyze_subdir_mapseq(proj_dir, genus, species):\n",
        "    # Iterate through subdirectories in proj_dir\n",
        "    for subdirectory in os.listdir(proj_dir):\n",
        "        subdir_path = os.path.join(proj_dir, subdirectory)\n",
        "\n",
        "        # Skip if not a directory\n",
        "        if not os.path.isdir(subdir_path):\n",
        "            print(f\"Skipping non-directory: {subdirectory}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Analyzing subdirectory {subdirectory} for MAPSeq files\")\n",
        "\n",
        "        # Check for required files and txt file\n",
        "        for file_name in os.listdir(subdir_path):\n",
        "            if file_name.endswith(\"SSU_MAPSeq.mseq\"):\n",
        "                print(file_name)\n",
        "\n",
        "                find_target_in_mapseq(subdir_path, subdir_path+\"/\"+file_name, genus, species)\n",
        "                print(\"Truncated strings obtained from MAPSeq file.\")\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def analyze_subdir_fasta(proj_dir):\n",
        "    # Iterate through subdirectories in proj_dir\n",
        "    for subdirectory in os.listdir(proj_dir):\n",
        "        subdir_path = os.path.join(proj_dir, subdirectory)\n",
        "\n",
        "        print(\"subdirectory path:\", subdir_path)\n",
        "\n",
        "        # Skip if not a directory\n",
        "        if not os.path.isdir(subdir_path):\n",
        "            print(f\"Skipping non-directory: {subdirectory}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Analyzing subdirectory {subdirectory} for FASTA files.\")\n",
        "\n",
        "        fasta_files = [file_name for file_name in os.listdir(subdir_path) if file_name.endswith(\"SSU.fasta\")]\n",
        "\n",
        "        # Check if any FASTA files are found in the subdirectory\n",
        "        if fasta_files:\n",
        "            # Iterate through FASTA files in the subdirectory\n",
        "            for fasta_file in fasta_files:\n",
        "                fasta_filepath = os.path.join(subdir_path, fasta_file)\n",
        "                print(\"Found FASTA file:\", fasta_filepath)\n",
        "\n",
        "                # Construct the path for the truncated strings file\n",
        "                truncated_strings_filepath = os.path.join(subdir_path, \"truncated_strings.txt\")\n",
        "\n",
        "                # Perform analysis using sift_fasta and save_sifted_data_to_csv functions\n",
        "                sifted_data = sift_fasta(fasta_filepath, truncated_strings_filepath)\n",
        "\n",
        "                # Create a new CSV filename based on the original FASTA filename\n",
        "                csv_filename = os.path.splitext(fasta_file)[0] + \"_extracted_sequences.csv\"\n",
        "\n",
        "                # Save the sifted data to the new CSV file\n",
        "                save_sifted_data_to_csv(sifted_data, os.path.join(subdir_path, csv_filename))\n",
        "\n",
        "            print(\"Extraction and CSV creation complete for all FASTA files.\")\n",
        "        else:\n",
        "            print(\"No FASTA files found in the subdirectory.\")\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def check_for_csv(subdirectory_path):\n",
        "    csv_files = [file for file in os.listdir(subdirectory_path) if file.endswith('.csv')]\n",
        "    return len(csv_files) > 0\n",
        "\n",
        "##################################################################################################################################\n",
        "\n",
        "def check_dir(root_directory):\n",
        "    subdirectories = [d for d in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory, d))]\n",
        "\n",
        "    presence_data = []\n",
        "    for subdir in subdirectories:\n",
        "        has_csv = check_for_csv(os.path.join(root_directory, subdir))\n",
        "        presence_data.append(has_csv)\n",
        "\n",
        "    num_have_csv = presence_data.count(True)\n",
        "    num_not_have_csv = presence_data.count(False)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.bar(['Sequences Found', 'Sequences Not Found'], [num_have_csv, num_not_have_csv], color=['green', 'red'])\n",
        "    #plt.xlabel('Project Sub-Folders')\n",
        "    plt.ylabel('Number of Sub-folders')\n",
        "    plt.title('Sub-folders Containing Sequences')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(root_directory + \"/sifting_results\")\n",
        "\n",
        "##################################################################################################################################\n",
        "\n",
        "def combine_csv_files(root_directory):\n",
        "    subdirectories = [d for d in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory, d))]\n",
        "\n",
        "    combined_sequences = {}\n",
        "\n",
        "    for subdir in subdirectories:\n",
        "        if check_for_csv(os.path.join(root_directory, subdir)):\n",
        "            subdir_sequences = []\n",
        "            for csv_file in os.listdir(os.path.join(root_directory, subdir)):\n",
        "                if csv_file.endswith('.csv'):\n",
        "                    csv_path = os.path.join(root_directory, subdir, csv_file)\n",
        "                    csv_content = pd.read_csv(csv_path)\n",
        "                    if 'Sequences' in csv_content.columns:\n",
        "                        sequences_data = csv_content['Sequences']\n",
        "                        subdir_sequences.extend(sequences_data)\n",
        "\n",
        "            if subdir_sequences:\n",
        "                combined_sequences[subdir] = subdir_sequences\n",
        "\n",
        "    if combined_sequences:\n",
        "        fasta_content = \"\"\n",
        "        for subdir, sequences in combined_sequences.items():\n",
        "            if len(sequences) == 1:\n",
        "                fasta_content += f\">{subdir}\\n{sequences[0]}\\n\"\n",
        "            else:\n",
        "                for i, sequence in enumerate(sequences, start=1):\n",
        "                    fasta_content += f\">{subdir}_{i}\\n{sequence}\\n\"\n",
        "\n",
        "        fasta_path = os.path.join(root_directory, 'seq_master.fasta')\n",
        "        with open(fasta_path, 'w') as fasta_file:\n",
        "            fasta_file.write(fasta_content)\n",
        "        print(\"Combined sequences saved as 'seq_master.fasta' in the root directory.\")\n",
        "\n",
        "        return fasta_path\n",
        "\n",
        "    else:\n",
        "        print(\"No sequences found in subdirectories.\")\n",
        "\n",
        "##################################################################################################################################\n",
        "\n",
        "def seq_master_lengths(fasta_file, root_directory):\n",
        "    # Check if the FASTA file exists\n",
        "    if not os.path.exists(fasta_file):\n",
        "        print(f\"Error: The file '{fasta_file}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    # Read sequences from the FASTA file\n",
        "    sequences = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
        "\n",
        "    # Get sequence names and lengths\n",
        "    seq_names = [seq.id for seq in sequences]\n",
        "    seq_lengths = [len(seq) for seq in sequences]\n",
        "\n",
        "    # Create a bar plot to visualize sequence lengths\n",
        "    plt.figure(figsize=(15, 9))\n",
        "    plt.bar(seq_names, seq_lengths, color='blue')\n",
        "    plt.xlabel('Sequence Names')\n",
        "    plt.ylabel('Sequence Length')\n",
        "    plt.title('Sequence Lengths from FASTA File')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(root_directory, \"sequence_lengths.png\"))\n",
        "\n",
        "##################################################################################################################################\n",
        "\n",
        "def analyze_nucleotide_freqs(fasta_file, root_directory):\n",
        "\n",
        "    sequences = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
        "\n",
        "    total_sequence = \"\".join(str(seq_record.seq) for seq_record in sequences)\n",
        "\n",
        "    # Calculate nucleotide frequencies for the entire fasta file\n",
        "    nucleotide_freq = {\n",
        "        'A': total_sequence.count('A'),\n",
        "        'T': total_sequence.count('T'),\n",
        "        'C': total_sequence.count('C'),\n",
        "        'G': total_sequence.count('G')\n",
        "    }\n",
        "\n",
        "    # Define colors for each nucleotide\n",
        "    colors = ['blue', 'orange', 'green', 'red']\n",
        "\n",
        "    # Plot nucleotide frequencies with colored bars\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    bars = plt.bar(nucleotide_freq.keys(), nucleotide_freq.values(), color=colors)\n",
        "    plt.xlabel('Nucleotides')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Overall Nucleotide Frequencies')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    total_count = sum(nucleotide_freq.values())\n",
        "    print(\"Overall Nucleotide Frequencies:\")\n",
        "    for nucleotide, count in nucleotide_freq.items():\n",
        "        print(f\"{nucleotide}: {count} ({(count / total_count) * 100:.2f}%)\")\n",
        "    plt.savefig(root_directory + \"/nucleotide_frequencies\")\n",
        "\n",
        "##################################################################################################################################"
      ],
      "metadata": {
        "id": "G6x8iqLtm7Wa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Main Function"
      ],
      "metadata": {
        "id": "vRPRM-5as6Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    print(\"Welcome to the GbIf Next-gen Sequence Analyzer! (GINSA)\")\n",
        "\n",
        "    proj_dir = input(\"Enter the path to an empty folder for this project:\")\n",
        "\n",
        "    species_name = input(\"Enter your target genus and species (i.e. Lecudina longissima):\")\n",
        "\n",
        "    gen_sp = species_name.split()\n",
        "\n",
        "    if len(gen_sp) == 2:\n",
        "        genus = gen_sp[0]\n",
        "        print(\"Genus: \", genus)\n",
        "        species = gen_sp[1]\n",
        "        print(\"Species: \", species)\n",
        "\n",
        "    elif len(gen_sp) == 1:\n",
        "        genus = gen_sp[0]\n",
        "        print(\"Genus: \", genus)\n",
        "        species = gen_sp[0]\n",
        "        print(\"Only Genus provided. Search will be based on: \", species)\n",
        "\n",
        "    elif len(gen_sp) > 2:\n",
        "        print(\"Too many search terms.\")\n",
        "\n",
        "    else:\n",
        "        print(\"No search taxon entered.\")\n",
        "\n",
        "    occurrence_ids = search_species_occurrences(species_name)\n",
        "\n",
        "    num_occurrences = len(occurrence_ids)\n",
        "\n",
        "    if num_occurrences > 0:\n",
        "        print(f\"Number of occurrences found: {num_occurrences}\")\n",
        "\n",
        "    else:\n",
        "        print(\"No occurrences found for this taxon.\")\n",
        "\n",
        "    if num_occurrences > 300:\n",
        "        print(\"There seem to be many ocurrences of this taxon in GBIF. Please ensure that you have sufficient storage.\")\n",
        "\n",
        "    create_folders(occurrence_ids, proj_dir)\n",
        "    print(\"Subdirectory folders created\")\n",
        "\n",
        "    print(\"Gathering biogeography data on all the occurrences found in GBIF...\")\n",
        "    generate_csv(occurrence_ids, proj_dir)\n",
        "\n",
        "    print(\"Occurrence metadata CSV created\")\n",
        "    csv_file = proj_dir+\"/occurrences.csv\"\n",
        "\n",
        "    print(\"Grabbing FASTA and MAPSeq files from MGnify via GBIF...\")\n",
        "    ssu_fasta_grab(csv_file, proj_dir)\n",
        "    mapseq_grab(csv_file, proj_dir)\n",
        "\n",
        "    print(\"Unpacking compressed files...\")\n",
        "    process_directory(proj_dir)\n",
        "\n",
        "    print(\"Searching MAPSeq files...\")\n",
        "    analyze_subdir_mapseq(proj_dir, genus, species)\n",
        "\n",
        "    print(\"Searching FASTA files...\")\n",
        "    analyze_subdir_fasta(proj_dir)\n",
        "\n",
        "    print(\"Sequence acquisition complete! Analyzing your sequences and saving them to a FASTA file.\")\n",
        "\n",
        "    print(\"Checking sub-folders for new sequences.\")\n",
        "    check_dir(proj_dir)\n",
        "\n",
        "    print(\"Aggregaring sequences into a master csv file.\")\n",
        "    fasta_path = combine_csv_files(proj_dir)\n",
        "\n",
        "    print(\"Generating a master FASTA file containing your new sequences.\")\n",
        "    seq_master_lengths(fasta_path, proj_dir)\n",
        "\n",
        "    print(\"Analyzing the sequences in your master FASTA file.\")\n",
        "    analyze_nucleotide_freqs(fasta_path, proj_dir)"
      ],
      "metadata": {
        "id": "vubNlT4Lm9b8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Main Function"
      ],
      "metadata": {
        "id": "gMNApdposvIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "krX2DG5_s-hJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}