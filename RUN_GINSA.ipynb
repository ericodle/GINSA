{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/7paBj4y6vxSBffSOQLTu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericodle/GINSA/blob/main/RUN_GINSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required packages"
      ],
      "metadata": {
        "id": "W0K3z1mbaVCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install wget\n",
        "!pip install biopython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcVcQ1ohtwrW",
        "outputId": "fe265e73-435e-42ea-db57-d37d07ce7b71"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=6fd248b5882a92450a7558c2c28ba7917dfcafb95516d794b14d2a73f453f711\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.22.4)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dependent packages"
      ],
      "metadata": {
        "id": "Pm5_dR1xcgxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import json\n",
        "import urllib.parse\n",
        "import os\n",
        "import csv\n",
        "import requests\n",
        "import pandas as pd\n",
        "import wget\n",
        "import gzip\n",
        "import shutil\n",
        "from Bio import SeqIO"
      ],
      "metadata": {
        "id": "614m_wYTce0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script functions"
      ],
      "metadata": {
        "id": "jU28G6draRyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################################################################\n",
        "\n",
        "def search_species_occurrences(species_name, limit=300):\n",
        "    base_url = \"https://api.gbif.org/v1/occurrence/search\"\n",
        "    params = {\n",
        "        \"scientificName\": species_name,\n",
        "        \"limit\": limit,\n",
        "    }\n",
        "\n",
        "    occurrence_ids = []\n",
        "\n",
        "    offset = 0\n",
        "    while True:\n",
        "        params[\"offset\"] = offset\n",
        "        encoded_params = urllib.parse.urlencode(params)\n",
        "        full_url = f\"{base_url}?{encoded_params}\"\n",
        "\n",
        "        with urllib.request.urlopen(full_url) as response:\n",
        "            data = json.loads(response.read())\n",
        "\n",
        "            if \"results\" in data:\n",
        "                occurrence_ids.extend([occurrence[\"key\"] for occurrence in data[\"results\"]])\n",
        "                if len(data[\"results\"]) < limit:\n",
        "                    break  # Reached the end of results\n",
        "                else:\n",
        "                    offset += limit\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    return occurrence_ids\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def create_folders(occurrence_ids, proj_dir):\n",
        "    if not os.path.exists(proj_dir):\n",
        "        os.makedirs(proj_dir)\n",
        "\n",
        "    for occurrence_id in occurrence_ids:\n",
        "        folder_path = os.path.join(proj_dir, str(occurrence_id))\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def generate_csv(occurrence_ids, proj_dir):\n",
        "    with open(proj_dir+'/occurrences.csv', mode='w', newline='') as csv_file:\n",
        "        fieldnames = ['occurrence_id', 'latitude', 'longitude', 'prefix_text']\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "\n",
        "        for occurrence_id in occurrence_ids:\n",
        "            url = f\"https://api.gbif.org/v1/occurrence/{occurrence_id}\"\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                try:\n",
        "                    data = response.json()\n",
        "                    long_id = data.get(\"occurrenceID\")\n",
        "                    latitude = data.get(\"decimalLatitude\")\n",
        "                    longitude = data.get(\"decimalLongitude\")\n",
        "                    underscore_index = long_id.find(\"_\")\n",
        "                    if underscore_index != -1:\n",
        "                        prefix_text = long_id[:underscore_index]\n",
        "                    else:\n",
        "                        prefix_text = \"N/A\"\n",
        "                    writer.writerow({\n",
        "                        'occurrence_id': occurrence_id,\n",
        "                        'latitude': latitude,\n",
        "                        'longitude': longitude,\n",
        "                        'prefix_text': prefix_text\n",
        "                    })\n",
        "                except ValueError:\n",
        "                    print(f\"Failed to parse JSON for occurrence ID {occurrence_id}.\")\n",
        "            else:\n",
        "                print(f\"Failed to fetch content for occurrence ID {occurrence_id}. Status code: {response.status_code}\")\n",
        "\n",
        "    print(\"CSV file created successfully.\")\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def process_directory(proj_dir):\n",
        "    for root, dirs, files in os.walk(proj_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".gz\"):\n",
        "                gz_file_path = os.path.join(root, file)\n",
        "                extract_path = os.path.splitext(gz_file_path)[0]\n",
        "\n",
        "                with gzip.open(gz_file_path, 'rb') as f_in, open(extract_path, 'wb') as f_out:\n",
        "                    shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "                os.remove(gz_file_path)\n",
        "                print(f\"Extracted and deleted: {gz_file_path}\")\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def ssu_fasta_grab(csv_file, proj_dir):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Replace with your actual MGnify API endpoint\n",
        "    api_base_url = \"https://www.ebi.ac.uk/metagenomics/api/v1\"\n",
        "\n",
        "    # Iterate through the DataFrame and process each occurrence\n",
        "    for prefix_text, occurrence_id in zip(df['prefix_text'], df['occurrence_id']):\n",
        "        url = f\"{api_base_url}/analyses/{prefix_text}/downloads\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "\n",
        "        print(f\"Processing {prefix_text}...\")\n",
        "\n",
        "        # Create a directory with the occurrence_id in the project directory\n",
        "        subdir_path = os.path.join(proj_dir, str(occurrence_id))\n",
        "        if not os.path.exists(subdir_path):\n",
        "            os.makedirs(subdir_path)\n",
        "\n",
        "        fasta_links = []\n",
        "\n",
        "        # Loop through the data to find links ending with \"MERGED_FASTQ_SSU.fasta.gz\"\n",
        "        for entry in data['data']:\n",
        "            link_entry = entry['links']['self']\n",
        "            if link_entry.endswith(\"MERGED_FASTQ_SSU.fasta.gz\"):\n",
        "                fasta_links.append(link_entry)\n",
        "\n",
        "        if fasta_links:\n",
        "            print(f\"Downloading {len(fasta_links)} file(s) for {prefix_text}...\")\n",
        "            for fasta_link in fasta_links:\n",
        "                file_name = os.path.basename(fasta_link)\n",
        "                file_path = os.path.join(subdir_path, file_name)\n",
        "                wget.download(fasta_link, file_path)\n",
        "            print(\"Download complete.\")\n",
        "        else:\n",
        "            print(f\"No fasta files found for {prefix_text}. Saving 'no_fasta.txt'...\")\n",
        "            no_fasta_file = os.path.join(subdir_path, \"no_fasta.txt\")\n",
        "            with open(no_fasta_file, \"w\") as f:\n",
        "                f.write(\"No fasta files found.\")\n",
        "\n",
        "        print()  # Add an empty line for better separation\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def mapseq_grab(csv_file, proj_dir):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Replace with your actual MGnify API endpoint\n",
        "    api_base_url = \"https://www.ebi.ac.uk/metagenomics/api/v1\"\n",
        "\n",
        "    # Iterate through the DataFrame and process each occurrence\n",
        "    for prefix_text, occurrence_id in zip(df['prefix_text'], df['occurrence_id']):\n",
        "        url = f\"{api_base_url}/analyses/{prefix_text}/downloads\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "\n",
        "        print(f\"Processing {prefix_text}...\")\n",
        "\n",
        "        # Create a directory with the occurrence_id in the project directory\n",
        "        subdir_path = os.path.join(proj_dir, str(occurrence_id))\n",
        "        if not os.path.exists(subdir_path):\n",
        "            os.makedirs(subdir_path)\n",
        "\n",
        "        mapseq_links = []\n",
        "\n",
        "        # Loop through the data to find links ending with \"MERGED_FASTQ_SSU_MAPSeq.mseq.gz\"\n",
        "        for entry in data['data']:\n",
        "            link_entry = entry['links']['self']\n",
        "            if link_entry.endswith(\"MERGED_FASTQ_SSU_MAPSeq.mseq.gz\"):\n",
        "                mapseq_links.append(link_entry)\n",
        "\n",
        "        if mapseq_links:\n",
        "            print(f\"Downloading {len(mapseq_links)} file(s) for {occurrence_id}...\")\n",
        "            for mapseq_link in mapseq_links:\n",
        "                file_name = os.path.basename(mapseq_link)\n",
        "                file_path = os.path.join(subdir_path, file_name)\n",
        "                wget.download(mapseq_link, file_path)\n",
        "            print(\"Download complete.\")\n",
        "        else:\n",
        "            print(f\"No MAPSeq files found for {occurrence_id}. Saving 'no_mapseq.txt'...\")\n",
        "            no_mapseq_file = os.path.join(subdir_path, \"no_mapseq.txt\")\n",
        "            with open(no_mapseq_file, \"w\") as f:\n",
        "                f.write(\"No MAPSeq files found.\")\n",
        "\n",
        "        print()  # Add an empty line for better separation\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def count_occurrences_in_mapseq_file(file_path, species):\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            content = file.read()\n",
        "\n",
        "            # Count the occurrences of the species name in the entire file\n",
        "            num_occurrences = content.count(species)\n",
        "\n",
        "            if num_occurrences == 0:\n",
        "                print(f\"'{species}' not found in the MAPSEQ file.\")\n",
        "            else:\n",
        "                print(f\"Number of occurrences of '{species}': {num_occurrences}\")\n",
        "\n",
        "            return num_occurrences\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: The file does not exist or the path is incorrect.\")\n",
        "        return 0\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return 0\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def find_target_in_mapseq(subdirectory, file_path, species):\n",
        "\n",
        "    truncated_strings = []  # List to store truncated strings\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            content = file.readlines()\n",
        "\n",
        "            for line in content:\n",
        "                line = line.strip()\n",
        "                if species in line:\n",
        "                    # Find the index of the search word in the line\n",
        "                    word_index = line.index(species)\n",
        "\n",
        "                    # Extract the preceding string\n",
        "                    preceding_string = line[:word_index].strip()\n",
        "                    # Truncate to only the text before the first space\n",
        "                    truncated_string = preceding_string.split()[0]\n",
        "                    truncated_strings.append(truncated_string)\n",
        "\n",
        "        with open(subdirectory+\"/truncated_strings.txt\", 'w') as output_file:\n",
        "            for item in truncated_strings:\n",
        "                output_file.write(\"%s\\n\" % item)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"MAPSeq Search Error: The file does not exist or the path is incorrect.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return truncated_strings\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def sift_fasta(fasta_file_path, truncated_strings):\n",
        "    sifted_data = {}  # Dictionary to store truncated_strings and their corresponding DNA sequences\n",
        "\n",
        "# Parse the FASTA file and extract sequences\n",
        "    with open(fasta_file_path, \"rt\") as fasta_file:\n",
        "        records = SeqIO.parse(fasta_file, \"fasta\")\n",
        "        for record in records:\n",
        "            sequence_name = record.id\n",
        "            sequence = str(record.seq)\n",
        "\n",
        "# Search for sequence names in the truncated_strings list\n",
        "        for truncated_string in truncated_strings:\n",
        "            if truncated_string in sequence_name:\n",
        "                sifted_data.setdefault(truncated_string, []).append(sequence)\n",
        "                break\n",
        "\n",
        "    return sifted_data\n",
        "###############################################################################################################################\n",
        "\n",
        "def save_sifted_data_to_csv(sifted_data, output_file):\n",
        "    with open(output_file, \"w\", newline=\"\") as csvfile:\n",
        "        csv_writer = csv.writer(csvfile)\n",
        "        csv_writer.writerow([\"Truncated String\", \"Sequences\"])\n",
        "\n",
        "        for truncated_string, sequences in sifted_data.items():\n",
        "            csv_writer.writerow([truncated_string, \"\\n\".join(sequences)])\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def analyze_subdir_mapseq(proj_dir, species):\n",
        "    # Iterate through subdirectories in proj_dir\n",
        "    for subdirectory in os.listdir(proj_dir):\n",
        "        subdir_path = os.path.join(proj_dir, subdirectory)\n",
        "\n",
        "        # Skip if not a directory\n",
        "        if not os.path.isdir(subdir_path):\n",
        "            continue\n",
        "\n",
        "        print(f\"Analyzing subdirectory: {subdirectory}\")\n",
        "\n",
        "        # Check for required files and txt file\n",
        "        for file_name in os.listdir(subdir_path):\n",
        "            if file_name.endswith(\"MERGED_FASTQ_SSU_MAPSeq.mseq\"):\n",
        "\n",
        "                print(file_name)\n",
        "              # Apply the count_occurrences_in_mapseq_file function\n",
        "                count_occurrences_in_mapseq_file(subdir_path+\"/\"+file_name, species)\n",
        "                find_target_in_mapseq(subdir_path, subdir_path+\"/\"+file_name, species)\n",
        "                print(\"Truncated strings obtained from MAPSeq file.\")\n",
        "            else:\n",
        "                print(\"No MAPSeq files found.\")\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "\n",
        "def analyze_subdir_fasta(proj_dir):\n",
        "    # Iterate through subdirectories in proj_dir\n",
        "    for subdirectory in os.listdir(proj_dir):\n",
        "        subdir_path = os.path.join(proj_dir, subdirectory)\n",
        "\n",
        "        print(\"subdirectory path:\", subdir_path)\n",
        "\n",
        "        # Skip if not a directory\n",
        "        if not os.path.isdir(subdir_path):\n",
        "            print(f\"Skipping non-directory: {subdirectory}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Analyzing subdirectory: {subdirectory}\")\n",
        "\n",
        "        fasta_files = [file_name for file_name in os.listdir(subdir_path) if file_name.endswith(\"MERGED_FASTQ_SSU.fasta\")]\n",
        "\n",
        "        # Check if any FASTA files are found in the subdirectory\n",
        "        if fasta_files:\n",
        "            # Iterate through FASTA files in the subdirectory\n",
        "            for fasta_file in fasta_files:\n",
        "                fasta_filepath = os.path.join(subdir_path, fasta_file)\n",
        "                print(\"Found FASTA file:\", fasta_filepath)\n",
        "\n",
        "                # Construct the path for the truncated strings file\n",
        "                truncated_strings_filepath = os.path.join(subdir_path, \"truncated_strings.txt\")\n",
        "\n",
        "                # Perform analysis using sift_fasta and save_sifted_data_to_csv functions\n",
        "                sifted_data = sift_fasta(fasta_filepath, truncated_strings_filepath)\n",
        "\n",
        "                # Create a new CSV filename based on the original FASTA filename\n",
        "                csv_filename = os.path.splitext(fasta_file)[0] + \"_extracted_sequences.csv\"\n",
        "\n",
        "                # Save the sifted data to the new CSV file\n",
        "                save_sifted_data_to_csv(sifted_data, os.path.join(subdir_path, csv_filename))\n",
        "\n",
        "            print(\"Extraction and CSV creation complete for all FASTA files.\")\n",
        "        else:\n",
        "            print(\"No FASTA files found in the subdirectory.\")\n",
        "\n",
        "###############################################################################################################################"
      ],
      "metadata": {
        "id": "6uhuMUO9klM-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main control function"
      ],
      "metadata": {
        "id": "XdP82_DtaNUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    print(\"Welcome to the GbIf Next-gen Sequence Analyzer! (GINSA)\")\n",
        "\n",
        "    proj_dir = input(\"Enter the path to an empty folder for this project.\")\n",
        "\n",
        "    species_name = input(\"Enter your target taxon in genus-species format (i.e. Lecudina tuzetae): \")\n",
        "\n",
        "    gen_sp = species_name.split()\n",
        "\n",
        "    genus = gen_sp[0]\n",
        "    print(\"Genus: \", genus)\n",
        "\n",
        "    species = gen_sp[1]\n",
        "    print(\"Species: \", species)\n",
        "\n",
        "    occurrence_ids = search_species_occurrences(species_name)\n",
        "\n",
        "    num_occurrences = len(occurrence_ids)\n",
        "\n",
        "    if num_occurrences > 0:\n",
        "        print(f\"Number of occurrences found: {num_occurrences}\")\n",
        "\n",
        "    else:\n",
        "        print(\"No occurrences found for the species.\")\n",
        "\n",
        "    create_folders(occurrence_ids, proj_dir)\n",
        "    print(\"Subdirectory folders created\")\n",
        "\n",
        "    generate_csv(occurrence_ids, proj_dir)\n",
        "    print(\"Occurrence metadata CSV created\")\n",
        "\n",
        "    csv_file = proj_dir+\"/occurrences.csv\"\n",
        "\n",
        "    print(\"Grabbing FASTA and MAPSeq files from GBIF and MGnify\")\n",
        "    ssu_fasta_grab(csv_file, proj_dir)\n",
        "\n",
        "    mapseq_grab(csv_file, proj_dir)\n",
        "\n",
        "\n",
        "    print(\"Unpacking compressed files\")\n",
        "    process_directory(proj_dir)\n",
        "\n",
        "    analyze_subdir_mapseq(proj_dir, species)\n",
        "\n",
        "    analyze_subdir_fasta(proj_dir)\n",
        "\n",
        "    print(\"Analysis complete! Sifted sequences can be found in their respective sub-folders\")"
      ],
      "metadata": {
        "id": "Twf-DQUMIOm-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the script!"
      ],
      "metadata": {
        "id": "BM0lxBJEaEwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "TRoOanq3dCqL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}