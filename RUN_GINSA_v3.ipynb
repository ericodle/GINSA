{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPv8B1vzot97B8GF7eRhjQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericodle/GINSA/blob/main/RUN_GINSA_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnxiphLXmtxY",
        "outputId": "147e229b-84b5-4018-ac91-d3594aceff2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=2558dbd813123db094bfbd88197f9f2a90b88d390147d062550f334dc5e3f012\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.22.4)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.81\n"
          ]
        }
      ],
      "source": [
        "!pip install requests\n",
        "!pip install wget\n",
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import json\n",
        "import urllib.parse\n",
        "import os\n",
        "import csv\n",
        "import requests\n",
        "import pandas as pd\n",
        "import wget\n",
        "import gzip\n",
        "import shutil\n",
        "from Bio import SeqIO\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Wtoaxix4m5Uo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################################################################\n",
        "\n",
        "def search_species_occurrences(species_name, limit=300):\n",
        "    base_url = \"https://api.gbif.org/v1/occurrence/search\"\n",
        "    params = {\n",
        "        \"scientificName\": species_name,\n",
        "        \"limit\": limit,\n",
        "    }\n",
        "\n",
        "    occurrence_ids = []\n",
        "\n",
        "    offset = 0\n",
        "    while True:\n",
        "        params[\"offset\"] = offset\n",
        "        encoded_params = urllib.parse.urlencode(params)\n",
        "        full_url = f\"{base_url}?{encoded_params}\"\n",
        "\n",
        "        with urllib.request.urlopen(full_url) as response:\n",
        "            data = json.loads(response.read())\n",
        "\n",
        "            if \"results\" in data:\n",
        "                occurrence_ids.extend([occurrence[\"key\"] for occurrence in data[\"results\"]])\n",
        "                if len(data[\"results\"]) < limit:\n",
        "                    break  # Reached the end of results\n",
        "                else:\n",
        "                    offset += limit\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    return occurrence_ids\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def create_folders(occurrence_ids, proj_dir):\n",
        "    if not os.path.exists(proj_dir):\n",
        "        os.makedirs(proj_dir)\n",
        "\n",
        "    for occurrence_id in occurrence_ids:\n",
        "        folder_path = os.path.join(proj_dir, str(occurrence_id))\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def generate_csv(occurrence_ids, proj_dir):\n",
        "    with open(proj_dir + '/occurrences.csv', mode='w', newline='', encoding='utf-8') as csv_file:\n",
        "        fieldnames = ['occurrence_id', 'country', 'latitude', 'longitude', 'prefix_text']\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "\n",
        "        for occurrence_id in occurrence_ids:\n",
        "            url = f\"https://api.gbif.org/v1/occurrence/{occurrence_id}\"\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                try:\n",
        "                    data = response.json()\n",
        "                    long_id = data.get(\"occurrenceID\")\n",
        "                    if long_id:\n",
        "                        country = data.get(\"countryCode\")\n",
        "                        latitude = data.get(\"decimalLatitude\")\n",
        "                        longitude = data.get(\"decimalLongitude\")\n",
        "                        underscore_index = long_id.find(\"_\")\n",
        "                        if underscore_index != -1:\n",
        "                            prefix_text = long_id[:underscore_index]\n",
        "                        else:\n",
        "                            prefix_text = \"N/A\"\n",
        "                        writer.writerow({\n",
        "                            'occurrence_id': occurrence_id,\n",
        "                            'country': country,\n",
        "                            'latitude': latitude,\n",
        "                            'longitude': longitude,\n",
        "                            'prefix_text': prefix_text\n",
        "                        })\n",
        "                    else:\n",
        "                        print(f\"occurrenceID not found for occurrence ID {occurrence_id}. Skipping.\")\n",
        "                except ValueError:\n",
        "                    print(f\"Failed to parse JSON for occurrence ID {occurrence_id}.\")\n",
        "            else:\n",
        "                print(f\"Failed to fetch content for occurrence ID {occurrence_id}. Status code: {response.status_code}\")\n",
        "\n",
        "    print(\"CSV file created successfully.\")\n",
        "###############################################################################################################################\n",
        "\n",
        "def process_directory(proj_dir):\n",
        "    for root, dirs, files in os.walk(proj_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".gz\"):\n",
        "                gz_file_path = os.path.join(root, file)\n",
        "                extract_path = os.path.splitext(gz_file_path)[0]\n",
        "\n",
        "                with gzip.open(gz_file_path, 'rb') as f_in, open(extract_path, 'wb') as f_out:\n",
        "                    shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "                os.remove(gz_file_path)\n",
        "                print(f\"Extracted and deleted: {gz_file_path}\")\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def ssu_fasta_grab(csv_file, proj_dir):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Replace with your actual MGnify API endpoint\n",
        "    api_base_url = \"https://www.ebi.ac.uk/metagenomics/api/v1\"\n",
        "\n",
        "    # Iterate through the DataFrame and process each occurrence\n",
        "    for prefix_text, occurrence_id in zip(df['prefix_text'], df['occurrence_id']):\n",
        "\n",
        "        if pd.isna(prefix_text):  # Check if prefix_text is NaN\n",
        "            print(\"MGnify link not found in occurrence metadata.\")\n",
        "            return\n",
        "\n",
        "        if \"MGY\" not in prefix_text.upper():  # Confirm prefix text points to EMBI MGnify\n",
        "            print(\"MGnify link not found in occurrence metadata.\")\n",
        "            return\n",
        "\n",
        "        url = f\"{api_base_url}/analyses/{prefix_text}/downloads\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "\n",
        "        print(f\"Processing {prefix_text}...\")\n",
        "\n",
        "        # Create a directory with the occurrence_id in the project directory\n",
        "        subdir_path = os.path.join(proj_dir, str(occurrence_id))\n",
        "        if not os.path.exists(subdir_path):\n",
        "            os.makedirs(subdir_path)\n",
        "\n",
        "        fasta_links = []\n",
        "\n",
        "        for entry in data['data']:\n",
        "            link_entry = entry['links']['self']\n",
        "            if link_entry.endswith(\"MERGED_FASTQ_SSU.fasta.gz\"):\n",
        "                fasta_links.append(link_entry)\n",
        "\n",
        "        if fasta_links:\n",
        "            print(f\"Downloading {len(fasta_links)} file(s) for {prefix_text}...\")\n",
        "            for fasta_link in fasta_links:\n",
        "                file_name = os.path.basename(fasta_link)\n",
        "                file_path = os.path.join(subdir_path, file_name)\n",
        "                wget.download(fasta_link, file_path)\n",
        "            print(\"Download complete.\")\n",
        "        else:\n",
        "            print(f\"No fasta files found for {prefix_text}. Saving 'no_fasta.txt'...\")\n",
        "            no_fasta_file = os.path.join(subdir_path, \"no_fasta.txt\")\n",
        "            with open(no_fasta_file, \"w\") as f:\n",
        "                f.write(\"No fasta files found.\")\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def mapseq_grab(csv_file, proj_dir):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Replace with your actual MGnify API endpoint\n",
        "    api_base_url = \"https://www.ebi.ac.uk/metagenomics/api/v1\"\n",
        "\n",
        "    # Iterate through the DataFrame and process each occurrence\n",
        "    for prefix_text, occurrence_id in zip(df['prefix_text'], df['occurrence_id']):\n",
        "\n",
        "        if pd.isna(prefix_text):  # Check if prefix_text is NaN\n",
        "            print(\"MGnify link not found in occurrence metadata.\")\n",
        "            return\n",
        "\n",
        "        url = f\"{api_base_url}/analyses/{prefix_text}/downloads\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "\n",
        "        print(f\"Processing {prefix_text}...\")\n",
        "\n",
        "        # Create a directory with the occurrence_id in the project directory\n",
        "        subdir_path = os.path.join(proj_dir, str(occurrence_id))\n",
        "        if not os.path.exists(subdir_path):\n",
        "            os.makedirs(subdir_path)\n",
        "\n",
        "        mapseq_links = []\n",
        "\n",
        "\n",
        "        # Check if 'data' key is present in the response dictionary\n",
        "        if 'data' not in data:\n",
        "            print(\"Error: 'data' key not present in the API response.\")\n",
        "            return\n",
        "\n",
        "        # Loop through the data to find links ending with \"MERGED_FASTQ_SSU_MAPSeq.mseq.gz\"\n",
        "        for entry in data['data']:\n",
        "            link_entry = entry['links']['self']\n",
        "            if link_entry.endswith(\"MERGED_FASTQ_SSU_MAPSeq.mseq.gz\"):\n",
        "                mapseq_links.append(link_entry)\n",
        "\n",
        "        if mapseq_links:\n",
        "            print(f\"Downloading {len(mapseq_links)} file(s) for {occurrence_id}...\")\n",
        "            for mapseq_link in mapseq_links:\n",
        "                file_name = os.path.basename(mapseq_link)\n",
        "                file_path = os.path.join(subdir_path, file_name)\n",
        "                wget.download(mapseq_link, file_path)\n",
        "            print(\"Download complete.\")\n",
        "        else:\n",
        "            print(f\"No MAPSeq files found for {occurrence_id}. Saving 'no_mapseq.txt'...\")\n",
        "            no_mapseq_file = os.path.join(subdir_path, \"no_mapseq.txt\")\n",
        "            with open(no_mapseq_file, \"w\") as f:\n",
        "                f.write(\"No MAPSeq files found.\")\n",
        "\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def count_occurrences_in_mapseq_file(file_path, species):\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            content = file.read()\n",
        "\n",
        "            # Count the occurrences of the species name in the entire file\n",
        "            num_occurrences = content.count(species)\n",
        "\n",
        "            if num_occurrences == 0:\n",
        "                print(f\"'{species}' not found in the MAPSEQ file.\")\n",
        "            else:\n",
        "                print(f\"Number of occurrences of '{species}': {num_occurrences}\")\n",
        "\n",
        "            return num_occurrences\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: The file does not exist or the path is incorrect.\")\n",
        "        return 0\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return 0\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def find_target_in_mapseq(subdirectory, file_path, species):\n",
        "\n",
        "    truncated_strings = []  # List to store truncated strings\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            content = file.readlines()\n",
        "\n",
        "            for line in content:\n",
        "                line = line.strip()\n",
        "                if species in line:\n",
        "                    # Find the index of the search word in the line\n",
        "                    word_index = line.index(species)\n",
        "\n",
        "                    # Extract the preceding string\n",
        "                    preceding_string = line[:word_index].strip()\n",
        "                    # Truncate to only the text before the first space\n",
        "                    truncated_string = preceding_string.split()[0]\n",
        "                    truncated_strings.append(truncated_string)\n",
        "\n",
        "        with open(subdirectory+\"/truncated_strings.txt\", 'w') as output_file:\n",
        "            for item in truncated_strings:\n",
        "                output_file.write(\"%s\\n\" % item)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"MAPSeq Search Error: The file does not exist or the path is incorrect.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    return truncated_strings\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def sift_fasta(fasta_file_path, truncated_strings):\n",
        "    sifted_data = {}  # Dictionary to store truncated_strings and their corresponding DNA sequences\n",
        "\n",
        "# Parse the FASTA file and extract sequences\n",
        "    with open(fasta_file_path, \"rt\") as fasta_file:\n",
        "        records = SeqIO.parse(fasta_file, \"fasta\")\n",
        "        for record in records:\n",
        "            sequence_name = record.id\n",
        "            sequence = str(record.seq)\n",
        "\n",
        "# Search for sequence names in the truncated_strings list\n",
        "        for truncated_string in truncated_strings:\n",
        "            if truncated_string in sequence_name:\n",
        "                sifted_data.setdefault(truncated_string, []).append(sequence)\n",
        "                break\n",
        "\n",
        "    return sifted_data\n",
        "###############################################################################################################################\n",
        "\n",
        "def save_sifted_data_to_csv(sifted_data, output_file):\n",
        "    with open(output_file, \"w\", newline=\"\") as csvfile:\n",
        "        csv_writer = csv.writer(csvfile)\n",
        "        csv_writer.writerow([\"Truncated String\", \"Sequences\"])\n",
        "\n",
        "        for truncated_string, sequences in sifted_data.items():\n",
        "            csv_writer.writerow([truncated_string, \"\\n\".join(sequences)])\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "def analyze_subdir_mapseq(proj_dir, species):\n",
        "    # Iterate through subdirectories in proj_dir\n",
        "    for subdirectory in os.listdir(proj_dir):\n",
        "        subdir_path = os.path.join(proj_dir, subdirectory)\n",
        "\n",
        "        # Skip if not a directory\n",
        "        if not os.path.isdir(subdir_path):\n",
        "            continue\n",
        "\n",
        "        print(f\"Analyzing subdirectory: {subdirectory}\")\n",
        "\n",
        "        # Check for required files and txt file\n",
        "        for file_name in os.listdir(subdir_path):\n",
        "            if file_name.endswith(\"MERGED_FASTQ_SSU_MAPSeq.mseq\"):\n",
        "\n",
        "                print(file_name)\n",
        "              # Apply the count_occurrences_in_mapseq_file function\n",
        "                count_occurrences_in_mapseq_file(subdir_path+\"/\"+file_name, species)\n",
        "                find_target_in_mapseq(subdir_path, subdir_path+\"/\"+file_name, species)\n",
        "                print(\"Truncated strings obtained from MAPSeq file.\")\n",
        "            else:\n",
        "                print(\"No MAPSeq files found.\")\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "\n",
        "def analyze_subdir_fasta(proj_dir):\n",
        "    # Iterate through subdirectories in proj_dir\n",
        "    for subdirectory in os.listdir(proj_dir):\n",
        "        subdir_path = os.path.join(proj_dir, subdirectory)\n",
        "\n",
        "        print(\"subdirectory path:\", subdir_path)\n",
        "\n",
        "        # Skip if not a directory\n",
        "        if not os.path.isdir(subdir_path):\n",
        "            print(f\"Skipping non-directory: {subdirectory}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Analyzing subdirectory: {subdirectory}\")\n",
        "\n",
        "        fasta_files = [file_name for file_name in os.listdir(subdir_path) if file_name.endswith(\"MERGED_FASTQ_SSU.fasta\")]\n",
        "\n",
        "        # Check if any FASTA files are found in the subdirectory\n",
        "        if fasta_files:\n",
        "            # Iterate through FASTA files in the subdirectory\n",
        "            for fasta_file in fasta_files:\n",
        "                fasta_filepath = os.path.join(subdir_path, fasta_file)\n",
        "                print(\"Found FASTA file:\", fasta_filepath)\n",
        "\n",
        "                # Construct the path for the truncated strings file\n",
        "                truncated_strings_filepath = os.path.join(subdir_path, \"truncated_strings.txt\")\n",
        "\n",
        "                # Perform analysis using sift_fasta and save_sifted_data_to_csv functions\n",
        "                sifted_data = sift_fasta(fasta_filepath, truncated_strings_filepath)\n",
        "\n",
        "                # Create a new CSV filename based on the original FASTA filename\n",
        "                csv_filename = os.path.splitext(fasta_file)[0] + \"_extracted_sequences.csv\"\n",
        "\n",
        "                # Save the sifted data to the new CSV file\n",
        "                save_sifted_data_to_csv(sifted_data, os.path.join(subdir_path, csv_filename))\n",
        "\n",
        "            print(\"Extraction and CSV creation complete for all FASTA files.\")\n",
        "        else:\n",
        "            print(\"No FASTA files found in the subdirectory.\")\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "\n",
        "def check_for_csv(subdirectory_path):\n",
        "    csv_files = [file for file in os.listdir(subdirectory_path) if file.endswith('.csv')]\n",
        "    return len(csv_files) > 0\n",
        "\n",
        "##################################################################################################################################\n",
        "\n",
        "def check_dir(root_directory):\n",
        "    subdirectories = [d for d in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory, d))]\n",
        "\n",
        "    presence_data = []\n",
        "    for subdir in subdirectories:\n",
        "        has_csv = check_for_csv(os.path.join(root_directory, subdir))\n",
        "        presence_data.append(has_csv)\n",
        "\n",
        "    num_have_csv = presence_data.count(True)\n",
        "    num_not_have_csv = presence_data.count(False)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.bar(['Sequences Found', 'Sequences Not Found'], [num_have_csv, num_not_have_csv], color=['green', 'red'])\n",
        "    #plt.xlabel('Project Sub-Folders')\n",
        "    plt.ylabel('Number of Sub-folders')\n",
        "    plt.title('Sub-folders Containing Sequences')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.savefig(root_directory + \"/sifting_results\")\n",
        "\n",
        "##################################################################################################################################\n",
        "\n",
        "def combine_csv_files(root_directory):\n",
        "    subdirectories = [d for d in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory, d))]\n",
        "\n",
        "    combined_sequences = {}\n",
        "\n",
        "    for subdir in subdirectories:\n",
        "        if check_for_csv(os.path.join(root_directory, subdir)):\n",
        "            subdir_sequences = []\n",
        "            for csv_file in os.listdir(os.path.join(root_directory, subdir)):\n",
        "                if csv_file.endswith('.csv'):\n",
        "                    csv_path = os.path.join(root_directory, subdir, csv_file)\n",
        "                    csv_content = pd.read_csv(csv_path)\n",
        "                    if 'Sequences' in csv_content.columns:\n",
        "                        sequences_data = csv_content['Sequences']\n",
        "                        subdir_sequences.extend(sequences_data)\n",
        "\n",
        "            if subdir_sequences:\n",
        "                combined_sequences[subdir] = subdir_sequences\n",
        "\n",
        "    if combined_sequences:\n",
        "        fasta_content = \"\"\n",
        "        for subdir, sequences in combined_sequences.items():\n",
        "            if len(sequences) == 1:\n",
        "                fasta_content += f\">{subdir}\\n{sequences[0]}\\n\"\n",
        "            else:\n",
        "                for i, sequence in enumerate(sequences, start=1):\n",
        "                    fasta_content += f\">{subdir}_{i}\\n{sequence}\\n\"\n",
        "\n",
        "        fasta_path = os.path.join(root_directory, 'seq_master.fasta')\n",
        "        with open(fasta_path, 'w') as fasta_file:\n",
        "            fasta_file.write(fasta_content)\n",
        "        print(\"Combined sequences saved as 'seq_master.fasta' in the root directory.\")\n",
        "\n",
        "        return fasta_path\n",
        "\n",
        "    else:\n",
        "        print(\"No sequences found in subdirectories.\")\n",
        "\n",
        "##################################################################################################################################\n",
        "\n",
        "def seq_master_lengths(fasta_file, root_directory):\n",
        "\n",
        "    # Read sequences from the FASTA file\n",
        "    sequences = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
        "\n",
        "    # Get sequence names and lengths\n",
        "    seq_names = [seq.id for seq in sequences]\n",
        "    seq_lengths = [len(seq) for seq in sequences]\n",
        "\n",
        "    # Create a bar plot to visualize sequence lengths\n",
        "    plt.figure(figsize=(15, 9))\n",
        "    plt.bar(seq_names, seq_lengths, color='blue')\n",
        "    plt.xlabel('Sequence Names')\n",
        "    plt.ylabel('Sequence Length')\n",
        "    plt.title('Sequence Lengths from FASTA File')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "    plt.savefig(root_directory + \"/sequence_lengths\")\n",
        "\n",
        "##################################################################################################################################\n",
        "\n",
        "def analyze_nucleotide_freqs(fasta_file, root_directory):\n",
        "\n",
        "    sequences = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
        "\n",
        "    total_sequence = \"\".join(str(seq_record.seq) for seq_record in sequences)\n",
        "\n",
        "    # Calculate nucleotide frequencies for the entire fasta file\n",
        "    nucleotide_freq = {\n",
        "        'A': total_sequence.count('A'),\n",
        "        'T': total_sequence.count('T'),\n",
        "        'C': total_sequence.count('C'),\n",
        "        'G': total_sequence.count('G')\n",
        "    }\n",
        "\n",
        "    # Define colors for each nucleotide\n",
        "    colors = ['blue', 'orange', 'green', 'red']\n",
        "\n",
        "    # Plot nucleotide frequencies with colored bars\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    bars = plt.bar(nucleotide_freq.keys(), nucleotide_freq.values(), color=colors)\n",
        "    plt.xlabel('Nucleotides')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Overall Nucleotide Frequencies')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    total_count = sum(nucleotide_freq.values())\n",
        "    print(\"Overall Nucleotide Frequencies:\")\n",
        "    for nucleotide, count in nucleotide_freq.items():\n",
        "        print(f\"{nucleotide}: {count} ({(count / total_count) * 100:.2f}%)\")\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "    plt.savefig(root_directory + \"/nucleotide_frequencies\")\n",
        "\n",
        "##################################################################################################################################"
      ],
      "metadata": {
        "id": "G6x8iqLtm7Wa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    print(\"Welcome to the GbIf Next-gen Sequence Analyzer! (GINSA)\")\n",
        "\n",
        "    proj_dir = input(\"Enter the path to an empty folder for this project.\")\n",
        "\n",
        "    species_name = input(\"Enter your target taxon in genus-species format (i.e. Lecudina tuzetae). You can also enter Genus alone: \")\n",
        "\n",
        "    gen_sp = species_name.split()\n",
        "\n",
        "    if len(gen_sp) == 2:\n",
        "        genus = gen_sp[0]\n",
        "        print(\"Genus: \", genus)\n",
        "        species = gen_sp[1]\n",
        "        print(\"Species: \", species)\n",
        "\n",
        "    elif len(gen_sp) == 1:\n",
        "        genus = gen_sp[0]\n",
        "        print(\"Genus: \", genus)\n",
        "        species = gen_sp[0]\n",
        "        print(\"Only Genus provided. Search will be based on: \", species)\n",
        "\n",
        "    elif len(gen_sp) > 2:\n",
        "        print(\"Too many search terms.\")\n",
        "\n",
        "    else:\n",
        "        print(\"No search taxon entered.\")\n",
        "\n",
        "    occurrence_ids = search_species_occurrences(species_name)\n",
        "\n",
        "    num_occurrences = len(occurrence_ids)\n",
        "\n",
        "    if num_occurrences > 0:\n",
        "        print(f\"Number of occurrences found: {num_occurrences}\")\n",
        "\n",
        "    else:\n",
        "        print(\"No occurrences found for this taxon.\")\n",
        "\n",
        "    if num_occurrences > 300:\n",
        "        print(\"There seem to be many ocurrences of this taxon in GBIF. Please ensure that you have sufficient storage.\")\n",
        "\n",
        "    create_folders(occurrence_ids, proj_dir)\n",
        "    print(\"Subdirectory folders created\")\n",
        "\n",
        "    print(\"Gathering biogeography data on all the occurrences found in GBIF...\")\n",
        "    generate_csv(occurrence_ids, proj_dir)\n",
        "\n",
        "    print(\"Occurrence metadata CSV created\")\n",
        "    csv_file = proj_dir+\"/occurrences.csv\"\n",
        "\n",
        "    print(\"Grabbing FASTA and MAPSeq files from MGnify via GBIF...\")\n",
        "    ssu_fasta_grab(csv_file, proj_dir)\n",
        "    mapseq_grab(csv_file, proj_dir)\n",
        "\n",
        "    print(\"Unpacking compressed files...\")\n",
        "    process_directory(proj_dir)\n",
        "\n",
        "    print(\"Searching MAPSeq files...\")\n",
        "    analyze_subdir_mapseq(proj_dir, species)\n",
        "\n",
        "    print(\"Searching FASTA files...\")\n",
        "    analyze_subdir_fasta(proj_dir)\n",
        "\n",
        "    print(\"Sequence acquisition complete! Analyzing your sequences and saving them to a FASTA file.\")\n",
        "\n",
        "    print(\"Checking sub-folders for new sequences.\")\n",
        "    check_dir(proj_dir)\n",
        "\n",
        "    print(\"Aggregaring sequences into a master csv file.\")\n",
        "    fasta_path = combine_csv_files(proj_dir)\n",
        "\n",
        "    print(\"Generating a master FASTA file containing your new sequences.\")\n",
        "    seq_master_lengths(fasta_path, proj_dir)\n",
        "\n",
        "    print(\"Analyzing the sequences in your master FASTA file.\")\n",
        "    analyze_nucleotide_freqs(fasta_path, proj_dir)"
      ],
      "metadata": {
        "id": "vubNlT4Lm9b8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qfFQLUomnAeW",
        "outputId": "c2e10ad8-c37f-461a-b14c-22e5b5cf8ed3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the GbIf Next-gen Sequence Analyzer! (GINSA)\n",
            "Enter the path to an empty folder for this project./content/1\n",
            "Enter your target taxon in genus-species format (i.e. Lecudina tuzetae). You can also enter Genus alone: Lecudina tuzetae\n",
            "Genus:  Lecudina\n",
            "Species:  tuzetae\n",
            "Number of occurrences found: 309\n",
            "There seem to be many ocurrences of this taxon in GBIF. Please ensure that you have sufficient storage.\n",
            "Subdirectory folders created\n",
            "Gathering biogeography data on all the occurrences found in GBIF...\n",
            "CSV file created successfully.\n",
            "Occurrence metadata CSV created\n",
            "Grabbing FASTA and MAPSeq files from MGnify via GBIF...\n",
            "Processing MGYA00572459...\n",
            "No fasta files found for MGYA00572459. Saving 'no_fasta.txt'...\n",
            "Processing MGYA00572459...\n",
            "No fasta files found for MGYA00572459. Saving 'no_fasta.txt'...\n",
            "Processing MGYA00572463...\n",
            "No fasta files found for MGYA00572463. Saving 'no_fasta.txt'...\n",
            "Processing MGYA00572463...\n",
            "No fasta files found for MGYA00572463. Saving 'no_fasta.txt'...\n",
            "Processing MGYA00572466...\n",
            "No fasta files found for MGYA00572466. Saving 'no_fasta.txt'...\n",
            "Processing MGYA00572466...\n",
            "No fasta files found for MGYA00572466. Saving 'no_fasta.txt'...\n",
            "Processing MGYA00284926...\n",
            "Downloading 1 file(s) for MGYA00284926...\n",
            "Download complete.\n",
            "Processing MGYA00284926...\n",
            "Downloading 1 file(s) for MGYA00284926...\n",
            "Download complete.\n",
            "Processing MGYA00285051...\n",
            "Downloading 1 file(s) for MGYA00285051...\n",
            "Download complete.\n",
            "Processing MGYA00285051...\n",
            "Downloading 1 file(s) for MGYA00285051...\n",
            "Download complete.\n",
            "Processing MGYA00285274...\n",
            "Downloading 1 file(s) for MGYA00285274...\n",
            "Download complete.\n",
            "Processing MGYA00285461...\n",
            "Downloading 1 file(s) for MGYA00285461...\n",
            "Download complete.\n",
            "Processing MGYA00285578...\n",
            "Downloading 1 file(s) for MGYA00285578...\n",
            "Download complete.\n",
            "Processing MGYA00285795...\n",
            "Downloading 1 file(s) for MGYA00285795...\n",
            "Download complete.\n",
            "Processing MGYA00285978...\n",
            "Downloading 1 file(s) for MGYA00285978...\n",
            "Download complete.\n",
            "Processing MGYA00285978...\n",
            "Downloading 1 file(s) for MGYA00285978...\n",
            "Download complete.\n",
            "Processing MGYA00288692...\n",
            "Downloading 1 file(s) for MGYA00288692...\n",
            "Download complete.\n",
            "Processing MGYA00288692...\n",
            "Downloading 1 file(s) for MGYA00288692...\n",
            "Download complete.\n",
            "Processing MGYA00288964...\n",
            "Downloading 1 file(s) for MGYA00288964...\n",
            "Download complete.\n",
            "Processing MGYA00288964...\n",
            "Downloading 1 file(s) for MGYA00288964...\n",
            "Download complete.\n",
            "Processing MGYA00289180...\n",
            "Downloading 1 file(s) for MGYA00289180...\n",
            "Download complete.\n",
            "Processing MGYA00289188...\n",
            "Downloading 1 file(s) for MGYA00289188...\n",
            "Download complete.\n",
            "Processing MGYA00289864...\n",
            "Downloading 1 file(s) for MGYA00289864...\n",
            "Download complete.\n",
            "Processing MGYA00290348...\n",
            "Downloading 1 file(s) for MGYA00290348...\n",
            "Download complete.\n",
            "Processing MGYA00290441...\n",
            "Downloading 1 file(s) for MGYA00290441...\n",
            "Download complete.\n",
            "Processing MGYA00290441...\n",
            "Downloading 1 file(s) for MGYA00290441...\n",
            "Download complete.\n",
            "Processing MGYA00285424...\n",
            "Downloading 1 file(s) for MGYA00285424...\n",
            "Download complete.\n",
            "Processing MGYA00285446...\n",
            "Downloading 1 file(s) for MGYA00285446...\n",
            "Download complete.\n",
            "Processing MGYA00285448...\n",
            "Downloading 1 file(s) for MGYA00285448...\n",
            "Download complete.\n",
            "Processing MGYA00285685...\n",
            "Downloading 1 file(s) for MGYA00285685...\n",
            "Download complete.\n",
            "Processing MGYA00285789...\n",
            "Downloading 1 file(s) for MGYA00285789...\n",
            "Download complete.\n",
            "Processing MGYA00289061...\n",
            "Downloading 1 file(s) for MGYA00289061...\n",
            "Download complete.\n",
            "Processing MGYA00289069...\n",
            "Downloading 1 file(s) for MGYA00289069...\n",
            "Download complete.\n",
            "Processing MGYA00289084...\n",
            "Downloading 1 file(s) for MGYA00289084...\n",
            "Download complete.\n",
            "Processing MGYA00289275...\n",
            "Downloading 1 file(s) for MGYA00289275...\n",
            "Download complete.\n",
            "Processing MGYA00289763...\n",
            "Downloading 1 file(s) for MGYA00289763...\n",
            "Download complete.\n",
            "Processing MGYA00284953...\n",
            "Downloading 1 file(s) for MGYA00284953...\n",
            "Download complete.\n",
            "Processing MGYA00284953...\n",
            "Downloading 1 file(s) for MGYA00284953...\n",
            "Download complete.\n",
            "Processing MGYA00285028...\n",
            "Downloading 1 file(s) for MGYA00285028...\n",
            "Download complete.\n",
            "Processing MGYA00285028...\n",
            "Downloading 1 file(s) for MGYA00285028...\n",
            "Download complete.\n",
            "Processing MGYA00285156...\n",
            "Downloading 1 file(s) for MGYA00285156...\n",
            "Download complete.\n",
            "Processing MGYA00285156...\n",
            "Downloading 1 file(s) for MGYA00285156...\n",
            "Download complete.\n",
            "Processing MGYA00285163...\n",
            "Downloading 1 file(s) for MGYA00285163...\n",
            "Download complete.\n",
            "Processing MGYA00285265...\n",
            "Downloading 1 file(s) for MGYA00285265...\n",
            "Download complete.\n",
            "Processing MGYA00285265...\n",
            "Downloading 1 file(s) for MGYA00285265...\n",
            "Download complete.\n",
            "Processing MGYA00285561...\n",
            "Downloading 1 file(s) for MGYA00285561...\n",
            "Download complete.\n",
            "Processing MGYA00285565...\n",
            "Downloading 1 file(s) for MGYA00285565...\n",
            "Download complete.\n",
            "Processing MGYA00285949...\n",
            "Downloading 1 file(s) for MGYA00285949...\n",
            "Download complete.\n",
            "Processing MGYA00285949...\n",
            "Downloading 1 file(s) for MGYA00285949...\n",
            "Download complete.\n",
            "Processing MGYA00288620...\n",
            "Downloading 1 file(s) for MGYA00288620...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-972361fa1b80>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-0580693f87da>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Grabbing FASTA and MAPSeq files from MGnify via GBIF...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mssu_fasta_grab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mmapseq_grab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6b19cd2314b3>\u001b[0m in \u001b[0;36mssu_fasta_grab\u001b[0;34m(csv_file, proj_dir)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfasta_link\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                 \u001b[0mwget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfasta_link\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Download complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wget.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, out, bar)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mbinurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mtmpfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mulib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmpfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    537\u001b[0m                                   '_open', req)\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1391\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1392\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}